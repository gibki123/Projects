{"cells":[{"cell_type":"markdown","source":["# Problem \n","Identify potentailly fraudulent transactions from the given dataset\n"],"metadata":{"id":"TK8pX1w8EFAh"}},{"cell_type":"markdown","metadata":{"id":"M1naBNA3JY-8"},"source":["# Assumptions - Made by Maciej Król\n","* Model shouldn't be overfitted to given customers and counterparties\n","* Make model as robust as it can be for new customers and transactions \n","* Make best score on cross validation 4:1 <br>\n","* 1 - fraudulent, 0 - normal\n","* Business case is probably to catch every single fraudulent transaction so \n","business metric should be to maximise a recall and have precision/bal_acc in mind.\n","* In real envirionment scenario there should be probably another model or rules which can recognize client connections and institutions associated with fraud possibility.\n","\n","# Thoughts - Made by Maciej Król\n","* Id's of customer could have some kind of relation with date, or age of clients. Without any additional information can't use that. I make an assumption that there is no any signifacnt information in customer id besides first letter \n","* Transaction amount is probably not normalised. It should be brought to one currency e.g. USD.\n","* TODO: For better transaction normalisation, maybe take avg salary in given currency/USD, hmmm <br> UPD: Almost no correlation with fraud flag, can be waste of time\n","* We have only 91 fraudulent transactions (probably need some kind of oversampling, remember of class_weights)  "]},{"cell_type":"markdown","metadata":{"id":"lelYiB5BJWJD"},"source":["# Imports/Options"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":28117,"status":"ok","timestamp":1679725971390,"user":{"displayName":"Daniel Szponar","userId":"13298831531680176967"},"user_tz":-60},"id":"cAgHmaWANT_y","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3c809119-9ea8-4b7d-eacd-07e3ea0414d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.6/76.6 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25henv: CLEARML_WEB_HOST=https://app.clear.ml\n","env: CLEARML_API_HOST=https://api.clear.ml\n","env: CLEARML_FILES_HOST=https://files.clear.ml\n","env: CLEARML_API_ACCESS_KEY=QZS6KLLLR4UR43EFQONG\n","env: CLEARML_API_SECRET_KEY=AHdLlidwgSXAjYm8ISxZd0b5mwkcUjMTjd2wARnFBtMqfO0unT\n"]}],"source":["!pip install -q clearml\n","!pip install -q binclass-tools\n","!pip install -q catboost\n","\n","# Env settings to retrieve artifacts from clear_ml server\n","# Clear ml - relatively new tool for MLOps \n","# We are operating on Task objects which have connection to our artifacts database\n","%env CLEARML_WEB_HOST=https://app.clear.ml\n","%env CLEARML_API_HOST=https://api.clear.ml\n","%env CLEARML_FILES_HOST=https://files.clear.ml\n","%env CLEARML_API_ACCESS_KEY=QZS6KLLLR4UR43EFQONG\n","%env CLEARML_API_SECRET_KEY=AHdLlidwgSXAjYm8ISxZd0b5mwkcUjMTjd2wARnFBtMqfO0unT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ubz1dfv3NdtO"},"outputs":[],"source":["from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = 'all'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dzuRg67jKOKr","colab":{"base_uri":"https://localhost:8080/","height":353},"executionInfo":{"status":"error","timestamp":1679725973037,"user_tz":-60,"elapsed":1659,"user":{"displayName":"Daniel Szponar","userId":"13298831531680176967"}},"outputId":"7024ca9f-0ca9-46f8-b4e6-3f5f796783c8"},"outputs":[{"output_type":"error","ename":"OptionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOptionError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-1a91303630fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'axes.titlesize'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'x-large'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'max_columns'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__func__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m_set_option\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_single_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_registered_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m_get_single_key\u001b[0;34m(pat, silent)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mOptionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No such keys(s): {repr(pat)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOptionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pattern matched multiple keys\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOptionError\u001b[0m: 'Pattern matched multiple keys'"]}],"source":["import pandas as pd\n","import numpy as np \n","\n","from typing import List, Dict\n","\n","import math\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","from sklearn.utils import shuffle\n","from sklearn.preprocessing import LabelEncoder\n","\n","from datetime import datetime\n","\n","from scipy.stats import pointbiserialr, chi2_contingency\n","\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.preprocessing import StandardScaler\n","\n","plt.rcParams['figure.figsize'] = [20, 12]\n","plt.rcParams['xtick.labelsize'] = 16\n","plt.rcParams['ytick.labelsize'] = 16\n","\n","plt.rcParams['axes.titlesize'] = 'x-large'\n","\n","pd.set_option('max_columns', 50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gkFXYmJZWvHe"},"outputs":[],"source":["## Important for data retrieval\n","from clearml import Task\n","# task = Task.init(project_name='Silent_Eight_Project', task_name='task_3')\n","task = Task.get_task(task_id='b2a51c5fb8fb4b8490da923feed5df23')"]},{"cell_type":"markdown","metadata":{"id":"B9Ywni1zKp9F"},"source":["# EDA + data preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dhtEQkUrKgF1"},"outputs":[],"source":["# Get local copy of dataset\n","df_path = task.artifacts['transactions'].get_local_copy()\n","df = pd.read_csv(df_path, parse_dates=['timestamp'])\n","df = df.sample(frac = 1)"]},{"cell_type":"code","source":["def print_section(text: str):\n","  final_text = \"| \"+text + \" |\"\n","  print('\\n')\n","  print('*' * len(final_text))\n","  print(final_text) \n","  print('*' * len(final_text))"],"metadata":{"id":"YTuk3Mx7a9GN"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u5LVWQORK5Le"},"outputs":[],"source":["print_section('Types in DataFrame')\n","df.dtypes\n","\n","print_section('Duplicated Rows')\n","df[df.duplicated()]\n","\n","print_section('Number of nulls')\n","df.isna().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Au1THrHlC-nu"},"outputs":[],"source":["# Fill counterparty_country by same records of counterparty\n","# This nulls could have some significant information, let's save them for later\n","df = df.sort_values(by=['counterparty', 'counterparty_country'])\n","df.loc[:, 'counterparty_filled'] = df[['counterparty', 'counterparty_country']].groupby('counterparty').fillna(method='ffill')\n","\n","df.loc[:, 'country_null'] = 0\n","df.loc[(df['counterparty_country'].isna()) & (~df['counterparty_filled'].isna()), 'country_null'] = 1\n","\n","df = df.drop(columns=['counterparty_country'])\n","\n","# All of this counterparties occurs only once\n","df[df['counterparty_filled'].isna()]\n","# df[df['counterparty']==78153912424955]\n","\n","#Fill them with Unknown label\n","df.loc[:, 'counterparty_filled'] = df['counterparty_filled'].fillna('Unknown')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pcbINY8z4o0S"},"outputs":[],"source":["print_section('Unique occurences per column')\n","df.nunique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HyOvaV4BF38H"},"outputs":[],"source":["# We got 4 types of customers (probably) :D\n","print_section('Unique values per first letters of customers')\n","print(df['customer'].str[:3].nunique())\n","print(df['customer'].str[:2].nunique())\n","print(df['customer'].str[:1].nunique())\n","\n","print_section('Count for types of customers')\n","df['customer'].str[:1].value_counts()\n","\n","#Extract type for another information\n","df.loc[:, 'customer_type'] = df['customer'].str[:1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fI-Q6RM0kV2N"},"outputs":[],"source":["print_section('Count for transaction types')\n","df['type'].value_counts()\n","\n","print_section('Count for customer countries')\n","df['customer_country'].value_counts()\n","\n","# No currencies of high risk countries https://ec.europa.eu/transparency/documents-register/detail?ref=C(2022)9649&lang=en\n","print_section('Count for currencies')\n","df['ccy'].value_counts()\n","\n","# No countries of high risk countries https://ec.europa.eu/transparency/documents-register/detail?ref=C(2022)9649&lang=en\n","print_section('Count for counterparty countries')\n","df['counterparty_filled'].value_counts()"]},{"cell_type":"code","source":["# Merge all USA tags\n","us_replace_dict = {\n","    'USA': 'US',\n","    'United States': 'US'\n","}\n","df['counterparty_filled'] = df['counterparty_filled'].replace(us_replace_dict)\n","print_section('Count for counterparty countries filled with replace')\n","df['counterparty_filled'].value_counts()"],"metadata":{"id":"46kBKb73fFMs"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MGLhUUR_GGK1"},"outputs":[],"source":["# Check occurences of each customer\n","# Distribution is quite normal, no outlying clients\n","print_section('Count for customers')\n","df['customer'].value_counts()\n","customer_frequncies = df['customer'].value_counts()\n","plt.hist(x=customer_frequncies.values, bins=30, )\n","plt.title('Distribution of Customers')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9J6zEajzH1HN"},"outputs":[],"source":["# Check occurences of each counterparty\n","# First look probably no outliers\n","print_section('Count for counterparties')\n","print(df['counterparty'].value_counts())\n","customer_frequncy = df['counterparty'].value_counts().values\n","plt.hist(x=customer_frequncy, bins=30)\n","plt.title('Distribution of counterparties')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I6l249h7oO6g"},"outputs":[],"source":["df['amount'].astype(float)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YbmK7JHApKlU"},"outputs":[],"source":["# I assume that I can just delete [',', '$', '£'] \n","df[~df['amount'].str.match(r'^[0-9]*\\.?[0-9]*$')]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k1EI23V2piJC"},"outputs":[],"source":["df['amount'] = df['amount'].str.replace(',', '', regex=False)\n","df['amount'] = df['amount'].str.replace('$', '', regex=False)\n","df['amount'] = df['amount'].str.replace('£', '', regex=False)\n","print_section('List with broken records after replacing')\n","df[~df['amount'].str.match(r'^[0-9]*\\.?[0-9]*$')]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i4oksu5lupF6"},"outputs":[],"source":["df['amount'] = df['amount'].astype(float)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jlxc88BawAIt"},"outputs":[],"source":["print_section('Count for years')\n","df['timestamp'].dt.year.value_counts()\n","\n","print_section('Count for months')\n","df['timestamp'].dt.month.value_counts()\n","\n","print_section('Count for days')\n","df['timestamp'].dt.day.value_counts()\n","\n","print_section('Count for weekdays')\n","df['timestamp'].dt.weekday.value_counts()\n","\n","print_section('Count for hours')\n","df['timestamp'].dt.hour.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j_ZV80QOyiKC"},"outputs":[],"source":["df['year'] = df['timestamp'].dt.year\n","df['month'] = df['timestamp'].dt.month\n","df['weekday'] = df['timestamp'].dt.weekday\n","df['weekday'] += 1\n","df['day'] = df['timestamp'].dt.day\n","df['hour'] = df['timestamp'].dt.hour\n","\n","df['timestamp'] = df['timestamp'].dt.date\n","\n","print(df['timestamp'].min())\n","print(df['timestamp'].max())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nHtdvcARN6V5"},"outputs":[],"source":["# Here is the section with normalizing amount whhich was sent in transaction every amount is normalised by USD value per given date\n","\n","# Prepocess rate dataframe\n","rates_path = task.artifacts['exchange_rates'].get_local_copy()\n","rates = pd.read_csv(rates_path, on_bad_lines='skip')\n","rates.columns = rates.loc[1]\n","rates = rates.drop(index=[0,1,2,3,4,5,6,7])\n","rates.columns = [col[:3] for col in rates.columns]\n","rates = rates.rename(columns={\"Cur\": \"timestamp\"})\n","rates = rates.ffill()\n","\n","# Get only interesting timeframe\n","rates = rates[(rates['timestamp'] >= '2021-01-01' ) & (rates['timestamp'] <= '2022-01-01')]\n","rates = rates.set_index('timestamp')\n","\n","# Get only interesting currencies\n","currencies = ['CNY', 'SGD', 'USD', 'JPY', 'HKD', 'GBP', 'BRL', 'INR', 'EUR']\n","rates = rates[currencies]\n","\n","# Get only one EUR\n","rates = rates.iloc[:, :9]\n","\n","# Make currency conversion\n","def conversion_to_USD(row: pd.Series) -> float:\n","  timestamp = row['timestamp']\n","  amount = row['amount']\n","  ccy = row['ccy']\n","\n","  rate = rates.loc[timestamp][ccy]\n","  return amount/rate\n","\n","df['timestamp'] = df['timestamp'].astype(str)\n","df['amount_converted'] = df.apply(lambda x: conversion_to_USD(x), axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zSbk8rWdUzdz"},"outputs":[],"source":["# Get numerical target\n","df['fraud_flag'] = df['fraud_flag'].replace('Y', 1)\n","df['fraud_flag'] = df['fraud_flag'].replace('N', 0)\n","\n","# Round amounts of transactions\n","df['amount_converted'] = df['amount_converted'].round(2)\n","df['amount'] = df['amount'].round(2)\n","\n","# Get dummies for better modelling\n","df = pd.get_dummies(df, columns=['customer_country', 'type', 'ccy', 'counterparty_filled', 'customer_type'], drop_first=True)\n","df = df.drop(columns=['customer', 'counterparty', 'timestamp', 'year'])\n","df = df.reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yu-NuUnRbIfu"},"outputs":[],"source":["# Implement sin/cos functions for time\n","# very nice method of transforming time data \n","# In this scale, 23:00 and 01:00 is much closer to each other\n","def transform_time(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:\n","  for col in cols:\n","    df[f'{col}_norm'] = 2 * math.pi * df[col] / df[col].max()\n","    df[f\"cos_{col}\"] = np.cos(df[f\"{col}_norm\"])\n","    df[f\"sin_{col}\"] = np.sin(df[f\"{col}_norm\"])\n","\n","    df = df.drop(columns=f'{col}_norm')\n","  return df\n","\n","df = transform_time(df, cols=['month', 'weekday', 'day', 'hour'])\n","df = df.drop(columns = ['month', 'weekday', 'day', 'hour'])\n","\n","# task.upload_artifact('full_modelling_data', df)"]},{"cell_type":"markdown","metadata":{"id":"R5XPhkP8nSIp"},"source":["# Corr + some more exploration"]},{"cell_type":"code","source":["task = Task.get_task(task_id='b2a51c5fb8fb4b8490da923feed5df23')\n","df = task.artifacts['full_modelling_data'].get()"],"metadata":{"id":"obTHUSmGkUSy"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JNi90bJlfRmo"},"outputs":[],"source":["# Nice, not too big correlation between variables, \n","# Bad, no correlations with target variable :(\n","corr = df.corr()\n","f, ax = plt.subplots(figsize=(20, 16))\n","mask = np.triu(np.ones_like(corr, dtype=bool))\n","cmap = sns.diverging_palette(230, 20, as_cmap=True)\n","sns.heatmap(corr, annot=True, mask = mask, cmap=cmap, fmt='.1f')\n","plt.title('Correlation heatmap pearson')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YvM_o249fbre"},"outputs":[],"source":["corr = df.corr('kendall')\n","f, ax = plt.subplots(figsize=(20, 16))\n","mask = np.triu(np.ones_like(corr, dtype=bool))\n","cmap = sns.diverging_palette(230, 20, as_cmap=True)\n","sns.heatmap(corr, annot=True, mask = mask, cmap=cmap, fmt='.1f')\n","plt.title('Correlation heatmap kendall')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yPJBLrMGiI3w"},"outputs":[],"source":["# proper corr for only target_variable\n","# Bad scores :(\n","cont_cols = ['amount', 'amount_converted', 'cos_month', 'sin_month', 'cos_weekday',\n","       'sin_weekday', 'cos_day', 'sin_day', 'cos_hour', 'sin_hour']\n","\n","print_section('Biserial Correlations')\n","for col in cont_cols:\n","  print(col)\n","  print(pointbiserialr(df['fraud_flag'], df[col]))\n","  print()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y9r2zuEOjCvY"},"outputs":[],"source":["# Check binary corr\n","# There are some significant relations \n","def check_binary_cor(col: str):\n","  crosstab = pd.crosstab(index=df['fraud_flag'], columns=df[col])\n","\n","  print(col)\n","  print('p-val:', chi2_contingency(crosstab)[1])\n","  print()\n","\n","\n","bin_cols = ['country_null', 'customer_country_UK', 'customer_country_US', 'type_DIVIDEND',\n","       'type_INTEREST', 'type_INVESTMENT', 'type_OTHER', 'type_PAYMENT',\n","       'type_TRANSFER', 'type_TT', 'ccy_CNY', 'ccy_EUR', 'ccy_GBP', 'ccy_HKD',\n","       'ccy_INR', 'ccy_JPY', 'ccy_SGD', 'ccy_USD', 'counterparty_filled_CN',\n","       'counterparty_filled_DE', 'counterparty_filled_FR',\n","       'counterparty_filled_HK', 'counterparty_filled_IN',\n","       'counterparty_filled_JP', 'counterparty_filled_SG',\n","       'counterparty_filled_UK', 'counterparty_filled_US', 'customer_type_K', 'customer_type_P',\n","       'customer_type_R']\n","\n","# p=value < 0.05 there is significant relation between two binary variables\n","print_section('p-values for correlation')\n","for col in bin_cols:\n","  check_binary_cor(col)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"asTty70umOKM"},"outputs":[],"source":["cols = ['type_DIVIDEND', 'type_INTEREST', 'type_INVESTMENT', 'type_PAYMENT', \n","        'counterparty_filled_CN', 'counterparty_filled_FR', 'counterparty_filled_HK',\n","        'counterparty_filled_JP', 'customer_type_K', 'customer_type_P', 'customer_type_R']\n","def print_crosstab(cols: List[str]):\n","  for col in cols:\n","    crosstab = pd.crosstab(index=df['fraud_flag'], columns=df[col])\n","    print(crosstab)\n","    print()\n","print_section('Cross tables for binary, correlated variables')\n","print_crosstab(cols)"]},{"cell_type":"markdown","metadata":{"id":"j57nW3y_nxUo"},"source":["# Baseline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BuGO4O2Zc-0F"},"outputs":[],"source":["import bctools as bc\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dqu7i9RIvSIJ"},"outputs":[],"source":["from sklearn.model_selection import cross_val_score, StratifiedKFold\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.tree import DecisionTreeClassifier, plot_tree\n","from sklearn.metrics import balanced_accuracy_score, confusion_matrix, roc_auc_score\n","from sklearn.compose import ColumnTransformer\n","from sklearn.utils.class_weight import compute_class_weight\n","\n","from typing import Tuple\n","\n","from clearml import OutputModel"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K2HKH5lHny3S"},"outputs":[],"source":["# first make a try on only binary features which have some kind of relation \n","X = df[['type_DIVIDEND', 'type_INTEREST', 'type_INVESTMENT', 'type_PAYMENT', \n","        'counterparty_filled_CN', 'counterparty_filled_FR', 'counterparty_filled_HK',\n","        'counterparty_filled_JP', 'customer_type_K', 'customer_type_P', 'customer_type_R']]\n","y = df['fraud_flag']\n","\n","\n","# For better score evaluation just hit this 5/10 or even more times\n","def build_model_on_binaries(model, X_train: pd.DataFrame, y_train: pd.Series, \n","                            threshold: float = None, log: bool = False) -> Tuple[float, float]:\n","\n","  pipeline = make_pipeline(model)\n","\n","  k_fold = StratifiedKFold(n_splits=5, shuffle=True)\n","  folds = k_fold.split(X_train, y_train)\n","  train_scores = []\n","  scores = []\n","  recalls = []\n","\n","  for k, (train, test) in enumerate(folds, start=1):\n","      pipeline.fit(X_train.iloc[train, :], y_train.iloc[train])\n","      train_preds = pipeline.predict(X_train.iloc[train, :])\n","      train_score = balanced_accuracy_score(y_train.iloc[train].values, train_preds)\n","      train_scores.append(train_score)\n","\n","      if threshold:\n","        y_preds = (pipeline.predict_proba(X_train.iloc[test, :])[:, 1] >= threshold).astype(bool)\n","      else:\n","        y_preds = pipeline.predict(X_train.iloc[test, :])\n","        \n","      cm = confusion_matrix(y_train.iloc[test].values, y_preds)\n","\n","      score = balanced_accuracy_score(y_train.iloc[test].values, y_preds)\n","      scores.append(score)\n","\n","      recall = cm[1][1]/(cm[1][1]+cm[1][0])\n","      recalls.append(recall)\n","      precision = cm[1][1]/(cm[0][1]+cm[1][1])\n","\n","      if log:\n","        print(f'Fold: {k}, train_balanced_accuracy: {train_score:.3f},  balanced_accuracy: {score:.3f}, ' +\n","              f'precision: {precision:.3f}, recall: {recall:.3f}')\n","        print(cm)\n","        print()\n","\n","        y_preds_proba = pipeline.predict_proba(X_train.iloc[test, :])[:,1]\n","        bc.curve_ROC_plot(true_y=y_train.iloc[test].values, predicted_proba=y_preds_proba)\n","        print()\n","\n","  pipeline.mean_train_score = np.mean(train_scores)\n","  pipeline.mean_score = np.mean(scores)\n","  pipeline.mean_recall = np.mean(recalls)\n","\n","  if log:\n","    print('\\n\\nbalanced_accuracy: %.3f +/- %.3f' %(pipeline.mean_score, np.std(scores)))\n","    print('recall: %.3f +/- %.3f' %(pipeline.mean_recall, np.std(recalls)))\n","\n","  return pipeline.mean_train_score, pipeline.mean_score, pipeline.mean_recall"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fEVz6AB2zZIf"},"outputs":[],"source":["# Setting threshold to around 0.4 quite obviously boost our scores\n","# Recall is around 0.8, with balanced accuracy 0.77 we should try to improve this with some better models\n","mean_train_score, mean_score, mean_recall = build_model_on_binaries(DecisionTreeClassifier(max_depth=10, class_weight='balanced'), X, y, threshold=0.4, log=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZptEO4fQlmGv"},"outputs":[],"source":["train_scores = []\n","scores = []\n","recalls = []\n","\n","# 30 times probably enough to get valid scores\n","# let's try to beat this score\n","for i in tqdm(range(30)):\n","  mean_train_score, mean_score, mean_recall = build_model_on_binaries(DecisionTreeClassifier(max_depth=10, class_weight='balanced'), X, y, threshold=0.4, log=False)\n","  scores.append(mean_score)\n","  recalls.append(mean_recall)\n","  train_scores.append(mean_train_score)\n","print()\n","print(f'Mean train balanced_acc: {np.mean(train_scores)}', f'Mean balanced_acc: {np.mean(scores)}', f'Mean recall: {np.mean(recalls)}')\n","\n","# At 10 max depth, last improvement on train data."]},{"cell_type":"markdown","metadata":{"id":"G3myDMl6nqQ8"},"source":["# Challengers"]},{"cell_type":"markdown","metadata":{"id":"29D0xUuDztXg"},"source":["## CatBoost"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mW8c6qUUimaX"},"outputs":[],"source":["# Catboost is very nice model to make fast research about best features based on casual model parameters.\n","# Especially in this kind of classification cases.\n","# We don't have to scale features or even preprocess, catboost handling everything for us \n","\n","import catboost\n","from catboost import (\n","    CatBoostClassifier, \n","    EShapCalcType, \n","    EFeaturesSelectionAlgorithm,\n","    Pool,\n","    cv\n",")\n","\n","from sklearn.utils import class_weight"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z9-wV73riqqU"},"outputs":[],"source":["X = df.drop(columns=['fraud_flag', 'amount'])\n","y = df['fraud_flag']\n","\n","class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y), y=y)\n","class_weights"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fXz6oVxuisMp"},"outputs":[],"source":["def train_catboost_on_cv():\n","  pool = Pool(data=X, label=y)\n","  params = {\n","      'iterations': 100,\n","      'depth': 4,\n","      'class_weights': class_weights,\n","      'loss_function': 'Logloss',\n","      'custom_metric': ['Recall', 'BalancedAccuracy'],\n","      'verbose': False,\n","    }\n","\n","  scores = cv(pool, params, fold_count=5, stratified=True, as_pandas=True)\n","  return scores"]},{"cell_type":"code","source":["scores = train_catboost_on_cv()\n","# Somehow scores can be only seen well from logging files :(\n","# And they seem really bad for now\n","# scores"],"metadata":{"id":"HRaxif36mkK1"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7r2oOyLIokgp"},"outputs":[],"source":["def select_features_with_kfold(X_train: pd.DataFrame, y_train: pd.Series, features_to_select: int, steps: int) -> Tuple[float, float]:\n","\n","  k_fold = StratifiedKFold(n_splits=5, shuffle=True)\n","  folds = k_fold.split(X_train, y_train)\n","      \n","  algorithm = EFeaturesSelectionAlgorithm.RecursiveByShapValues\n","  summaries = []\n","\n","  for k, (train, test) in enumerate(folds, start=1):\n","    train_pool = Pool(X_train.iloc[train, :], y_train.iloc[train])\n","    test_pool = Pool(X_train.iloc[test, :], y_train.iloc[test])\n","\n","    params = {\n","      'iterations': 600,\n","      'auto_class_weights': 'Balanced',\n","      'loss_function': 'Logloss',\n","      'custom_metric': ['Recall', 'BalancedAccuracy'],\n","      'verbose': False,\n","    }\n","\n","    model = CatBoostClassifier(**params)\n","\n","    summary = model.select_features(\n","      train_pool,\n","      eval_set=test_pool,\n","      features_for_select=list(range(X_train.shape[1])),\n","      num_features_to_select = features_to_select,\n","      steps=steps,\n","      algorithm=algorithm,\n","      shap_calc_type=EShapCalcType.Regular,\n","\n","    )\n","    summaries.append(summary)\n","\n","  return summaries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eZZQyVCwq9bP"},"outputs":[],"source":["# 10 features_to_select no enough boost\n","# 8 features_to_select quite okay\n","summaries = select_features_with_kfold(X, y, 8, 3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B7H3CF1fuxZ-"},"outputs":[],"source":["selected_features = [summary['selected_features_names'] for summary in summaries]\n","indiv_selected_features = set(sum(selected_features, []))\n","\n","indiv_selected_features"]},{"cell_type":"code","source":["# first make a try on only binary features which have some kind of relation \n","X = df[['type_DIVIDEND', 'type_INTEREST', 'type_INVESTMENT', 'type_PAYMENT', \n","        'counterparty_filled_CN', 'counterparty_filled_FR', 'counterparty_filled_HK',\n","        'counterparty_filled_JP', 'customer_type_K', 'customer_type_P', 'customer_type_R']]\n","y = df['fraud_flag']\n","\n","\n","# For better score evaluation just hit this 5/10 or even more times\n","def build_model_on_binaries(model, X_train: pd.DataFrame, y_train: pd.Series, \n","                            threshold: float = None, log: bool = False) -> Tuple[float, float]:\n","\n","  pipeline = make_pipeline(model)\n","\n","  k_fold = StratifiedKFold(n_splits=5, shuffle=True)\n","  folds = k_fold.split(X_train, y_train)\n","\n","\n","  for k, (train, test) in enumerate(folds, start=1):\n","      pipeline.fit(X_train.iloc[train, :], y_train.iloc[train])\n","      train_preds = pipeline.predict(X_train.iloc[train, :])\n","      train_score = balanced_accuracy_score(y_train.iloc[train].values, train_preds)\n","      train_scores.append(train_score)\n","\n","      if threshold:\n","        y_preds = (pipeline.predict_proba(X_train.iloc[test, :])[:, 1] >= threshold).astype(bool)\n","      else:\n","        y_preds = pipeline.predict(X_train.iloc[test, :])\n","        \n","      cm = confusion_matrix(y_train.iloc[test].values, y_preds)\n","\n","      score = balanced_accuracy_score(y_train.iloc[test].values, y_preds)\n","      scores.append(score)\n","\n","      recall = cm[1][1]/(cm[1][1]+cm[1][0])\n","      recalls.append(recall)\n","      precision = cm[1][1]/(cm[0][1]+cm[1][1])\n","\n","      if log:\n","        print(f'Fold: {k}, train_balanced_accuracy: {train_score:.3f},  balanced_accuracy: {score:.3f}, ' +\n","              f'precision: {precision:.3f}, recall: {recall:.3f}')\n","        print(cm)\n","        print()\n","\n","        y_preds_proba = pipeline.predict_proba(X_train.iloc[test, :])[:,1]\n","        bc.curve_ROC_plot(true_y=y_train.iloc[test].values, predicted_proba=y_preds_proba)\n","        print()\n","\n","  pipeline.mean_train_score = np.mean(train_scores)\n","  pipeline.mean_score = np.mean(scores)\n","  pipeline.mean_recall = np.mean(recalls)\n","\n","  if log:\n","    print('\\n\\nbalanced_accuracy: %.3f +/- %.3f' %(pipeline.mean_score, np.std(scores)))\n","    print('recall: %.3f +/- %.3f' %(pipeline.mean_recall, np.std(recalls)))\n","\n","  return pipeline.mean_train_score, pipeline.mean_score, pipeline.mean_recall"],"metadata":{"id":"93k7GAWLrPru"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gXcBFQ7Qvj9_"},"outputs":[],"source":["def train_catboost_on_cv(X, y, threshold=None, log=None):\n","  k_fold = StratifiedKFold(n_splits=5, shuffle=True)\n","  folds = k_fold.split(X, y)\n","\n","  train_scores = []\n","  scores = []\n","  recalls = []\n","\n","  params = {\n","    'iterations': 500,\n","    'loss_function': 'Logloss',\n","    'auto_class_weights': 'Balanced',\n","    'custom_metric': ['Recall', 'BalancedAccuracy'],\n","    'verbose': False,\n","  }\n","\n","  for k, (train, test) in enumerate(folds, start=1):\n","    train_pool = Pool(X.iloc[train, :], y.iloc[train])\n","    test_pool = Pool(X.iloc[test, :], y.iloc[test])\n","\n","    model = CatBoostClassifier(**params)\n","    model.fit(train_pool, eval_set=test_pool, use_best_model=True, verbose=False) \n","\n","    train_preds = model.predict(X.iloc[train, :])\n","    train_score = balanced_accuracy_score(y.iloc[train].values, train_preds)\n","    train_scores.append(train_score)\n","    \n","    if threshold:\n","      y_preds = (model.predict_proba(X.iloc[test, :])[:, 1] >= threshold).astype(bool)\n","    else:\n","      y_preds = model.predict(X.iloc[test, :])\n","      \n","    cm = confusion_matrix(y.iloc[test].values, y_preds)\n","\n","    score = balanced_accuracy_score(y.iloc[test].values, y_preds)\n","    scores.append(score)\n","\n","    recall = cm[1][1]/(cm[1][1]+cm[1][0])\n","    recalls.append(recall)\n","    precision = cm[1][1]/(cm[0][1]+cm[1][1])\n","\n","    if log:\n","      print(f'Fold: {k}, train_balanced_accuracy: {train_score:.3f},  balanced_accuracy: {score:.3f}, ' +\n","            f'precision: {precision:.3f}, recall: {recall:.3f}')\n","      print(cm)\n","      print()\n","\n","    mean_train_score = np.mean(train_scores)\n","    mean_score = np.mean(scores)\n","    mean_recall = np.mean(recalls)\n","\n","  return mean_train_score, mean_score, mean_recall"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JOTeS1yVvpHN"},"outputs":[],"source":["# quite nice\n","train_catboost_on_cv(X[indiv_selected_features], y, log=True, threshold=0.3)"]},{"cell_type":"code","source":["train_scores = []\n","scores = []\n","recalls = []\n","\n","# 30 times probably enough to get valid scores\n","# let's try to beat this score\n","for i in tqdm(range(30)):\n","  mean_train_score, mean_score, mean_recall = train_catboost_on_cv(X[indiv_selected_features], y, log=False, threshold=0.26)\n","  train_scores.append(mean_train_score)\n","  scores.append(mean_score)\n","  recalls.append(mean_recall)\n","print()\n","print(f'Mean train balanced_acc: {np.mean(train_scores)}', f'Mean balanced_acc: {np.mean(scores)}', f'Mean recall: {np.mean(recalls)}')"],"metadata":{"id":"fsbb3wwmsHG8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# TODOS + Final Notes"],"metadata":{"id":"4wn2F0nv15l9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"52TSz4wq17aQ"},"outputs":[],"source":["# Model can be improved with some hyperparameter changes and better features selection \n","# Probably some more column dropping and research on shap values can improve the final score\n","\n","# Final model fitted on the whole dataset would be quite resilient for different and new clients. \n","# In final dataframe there is no information about client numbers and counterparties\n","\n","# We could also check vif score for multicollinearity between variables\n","\n","# We should go back to initial Tree and try different predict_proba thresholds\n","\n","# Model seem to be overfitting to training data, still big gap between train_mean_score and test_score\n","# Prune estimators, change max_leaf_nodes, min_obs_per_leaf, depth for better generalization"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}